{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKpDO8uKqxVZ"
   },
   "source": [
    "# Project 3B: Learning Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5TFCpFm5Ywda"
   },
   "source": [
    "## Introduction\n",
    "In this project, we will implement the *backpropagation* algorithm and apply it to the same problem as Project 3A, i.e. recognizing handwritten numeric digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1697118234337,
     "user": {
      "displayName": "Arthur C.",
      "userId": "18023806256318143056"
     },
     "user_tz": 180
    },
    "id": "mxXdTfSYYwdb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "from scipy import optimize\n",
    "from scipy.io import loadmat\n",
    "import utils\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pth3nwQpYwde"
   },
   "source": [
    "## Neural Networks\n",
    "In Project 3A, we implemented *feedforward* propagation in neural networks to recognize handwritten digits with the given parameters. In this project, we will implement the *backpropagation* algorithm to train the parameters for the neural network. First, we will load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "executionInfo": {
     "elapsed": 415,
     "status": "ok",
     "timestamp": 1697118234742,
     "user": {
      "displayName": "Arthur C.",
      "userId": "18023806256318143056"
     },
     "user_tz": 180
    },
    "id": "Njzut0QLYwde"
   },
   "outputs": [],
   "source": [
    "#  training data stored in arrays X, y\n",
    "data = loadmat(os.path.join(\"data\", \"p3bdata1.mat\"))\n",
    "X, y = data[\"X\"], data[\"y\"].ravel()\n",
    "\n",
    "# set the zero digit to 0, rather than its mapped 10 in this dataset\n",
    "# This is an artifact due to the fact that this dataset was used in\n",
    "# MATLAB where there is no index 0\n",
    "y[y == 10] = 0\n",
    "\n",
    "# Number of training examples\n",
    "m = y.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y281sI7TYwdg"
   },
   "source": [
    "### Data visualization\n",
    "We will start by visualizing part of our training data using the `displayData` function, which is the same one we used in Project 3A. The dataset is also the same as in the last project.\n",
    "There are 5000 training examples in `p3data1.mat`, where each example is a grayscale image of 20x20 pixels. Each pixel is represented by a number (a float variable) indicating the intensity at that location. The 20x20 pixel network is extended into a vector of length 400. Each of these training examples is placed in a row of the `X` matrix. This makes the `X` matrix 5000x400, where each row is a training example with the image of the handwritten numeric digit.\n",
    "$$ X = \\begin{bmatrix} - \\: (x^{(1)})^T \\: - \\\\ -\\: (x^{(2)})^T \\:- \\\\ \\vdots \\\\ - \\: (x^{(m)})^T \\:- \\end{bmatrix} $$\n",
    "The second part of the training data is the `y` vector of dimension 5000x1 and contains the labels (the correct digit present in each image). By executing the cell below, we will see some examples of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 807
    },
    "executionInfo": {
     "elapsed": 3379,
     "status": "ok",
     "timestamp": 1697118238112,
     "user": {
      "displayName": "Arthur C.",
      "userId": "18023806256318143056"
     },
     "user_tz": 180
    },
    "id": "-aagLWscYwdh",
    "outputId": "20ecedc4-45ff-44dd-ff3e-1257e169b1fb"
   },
   "outputs": [],
   "source": [
    "# Randomly select 100 data points to display\n",
    "rand_indices = np.random.choice(m, 100, replace=False)\n",
    "sel = X[rand_indices, :]\n",
    "\n",
    "utils.displayData(sel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3YWOoiOIYwdj"
   },
   "source": [
    "### Representation of the model\n",
    "Our neural network is shown in the following figure.\n",
    "\n",
    "![](figures/neural_network.png)\n",
    "\n",
    "This model has 3 layers: An input layer (*input layer*), a hidden layer (*hidden layer*) and an output layer (*output layer*). Note that its inputs are the intensity values of the pixels in the digit image. Knowing that the images are 20x20, this will result in an input layer of 400 units (excluding an extra unit of *bias* which always generates +1). As previously mentioned, the training data will be loaded into the variables `X` and `y`.\n",
    "You have received a set of network parameters ($\\Theta^{(1)}$, $\\Theta^{(2)}$) that we have already trained. This is stored in the file `p3bweights.mat`. The next cell loads the parameters in `Theta1` and `Theta2`. The parameters have dimensions that are arranged for a neural network with 25 units in the second layer and 10 units in the output layer (corresponding to the 10 classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1697118238112,
     "user": {
      "displayName": "Arthur C.",
      "userId": "18023806256318143056"
     },
     "user_tz": 180
    },
    "id": "vuq2_RrfYwdj"
   },
   "outputs": [],
   "source": [
    "# Setup the parameters you will use for this exercise\n",
    "input_layer_size = 400  # 20x20 Input Images of Digits\n",
    "hidden_layer_size = 25  # 25 hidden units\n",
    "num_labels = 10  # 10 labels, from 0 to 9\n",
    "\n",
    "# Load the weights into variables Theta1 and Theta2\n",
    "weights = loadmat(os.path.join(\"data/\", \"p3bweights.mat\"))\n",
    "\n",
    "# Theta1 has size 25 x 401\n",
    "# Theta2 has size 10 x 26\n",
    "Theta1, Theta2 = weights[\"Theta1\"], weights[\"Theta2\"]\n",
    "\n",
    "# swap first and last columns of Theta2, due to legacy from MATLAB indexing,\n",
    "# since the weight file ex3weights.mat was saved based on MATLAB indexing\n",
    "Theta2 = np.roll(Theta2, 1, axis=0)\n",
    "\n",
    "# Unroll parameters\n",
    "nn_params = np.concatenate([Theta1.ravel(), Theta2.ravel()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NKOVSFgLYwdl"
   },
   "source": [
    "### Feedforward and cost function\n",
    "Now we will implement the cost function and the gradient for the neural network. First, we have the code for the `nnCostFunction` function in the next cell to return the cost.\n",
    "Note that the cost function for the neural network (without regularization) is:\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m}\\sum_{k=1}^{K} \\left[ - y_k^{(i)} \\log \\left( \\left( h_\\theta \\left( x^{(i)} \\right) \\right)_k \\right) - \\left( 1 - y_k^{(i)} \\right) \\log \\left( 1 - \\left( h_\\theta \\left( x^{(i)} \\right) \\right)_k \\right) \\right]$$\n",
    "where $h_\\theta \\left(x^{(i)} \\right)$ is calculated as shown in the neural network figure above and K = 10 is the number of possible labels. Note that $h_\\theta(x^{(i)})_k = a_k^{(3)}$ is the activation (output value) of the $k^{th}$ output unit. Also note that while the original labels (in the y variable) range from 0 to 9, for the purpose of training the neural network, we need to encode the labels as vectors containing only values of 0 or 1. Thus:\n",
    "$$ y =\n",
    "\\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\vdots \\\\ 0 \\end{bmatrix}, \\quad\n",
    "\\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}, \\quad \\cdots \\quad \\text{or} \\qquad\n",
    "\\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 1 \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "For this example, if $x^{(i)}$ is an image of the digit 5, then the corresponding $y^{(i)}$ (which you must use in the cost function) must be a vector of length 10 with $y_5 = 1$, and all other elements equal to 0.\n",
    "\n",
    "We will implement *feedforward* propagation that calculates $h_\\theta(x^{(i)})$ for each example $i$ and add up the cost of all the examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2ct2Oi3Ywdr"
   },
   "source": [
    "### Regularized cost function\n",
    "The cost function of the neural network with regularization is given by:\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m}\\sum_{k=1}^{K} \\left[ - y_k^{(i)} \\log \\left( \\left( h_\\theta \\left( x^{(i)} \\right) \\right)_k \\right) - \\left( 1 - y_k^{(i)} \\right) \\log \\left( 1 - \\left( h_\\theta \\left( x^{(i)} \\right) \\right)_k \\right) \\right] + \\frac{\\lambda}{2 m} \\left[ \\sum_{j=1}^{25} \\sum_{k=1}^{400} \\left( \\Theta_{j,k}^{(1)} \\right)^2 + \\sum_{j=1}^{10} \\sum_{k=1}^{25} \\left( \\Theta_{j,k}^{(2)} \\right)^2 \\right] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1697118238113,
     "user": {
      "displayName": "Arthur C.",
      "userId": "18023806256318143056"
     },
     "user_tz": 180
    },
    "id": "1dPYAdZhYwdm"
   },
   "outputs": [],
   "source": [
    "def nnCostFunction(\n",
    "    nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lambda_=0.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Implements the neural network cost function and gradient for a two layer neural\n",
    "    network which performs classification.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    nn_params : array_like\n",
    "        The parameters for the neural network which are \"unrolled\" into\n",
    "        a vector. This needs to be converted back into the weight matrices Theta1\n",
    "        and Theta2.\n",
    "\n",
    "    input_layer_size : int\n",
    "        Number of features for the input layer.\n",
    "\n",
    "    hidden_layer_size : int\n",
    "        Number of hidden units in the second layer.\n",
    "\n",
    "    num_labels : int\n",
    "        Total number of labels, or equivalently number of units in output layer.\n",
    "\n",
    "    X : array_like\n",
    "        Input dataset. A matrix of shape (m x input_layer_size).\n",
    "\n",
    "    y : array_like\n",
    "        Dataset labels. A vector of shape (m,).\n",
    "\n",
    "    lambda_ : float, optional\n",
    "        Regularization parameter.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    J : float\n",
    "        The computed value for the cost function at the current weight values.\n",
    "\n",
    "    grad : array_like\n",
    "        An \"unrolled\" vector of the partial derivatives of the concatenatation of\n",
    "        neural network weights Theta1 and Theta2.\n",
    "    \"\"\"\n",
    "    # Reshape nn_params back into the parameters Theta1 and Theta2, the weight matrices\n",
    "    # for our 2 layer neural network\n",
    "    Theta1 = np.reshape(\n",
    "        nn_params[: hidden_layer_size * (input_layer_size + 1)],\n",
    "        (hidden_layer_size, (input_layer_size + 1)),\n",
    "    )\n",
    "\n",
    "    Theta2 = np.reshape(\n",
    "        nn_params[(hidden_layer_size * (input_layer_size + 1)) :],\n",
    "        (num_labels, (hidden_layer_size + 1)),\n",
    "    )\n",
    "\n",
    "    # Setup some useful variables\n",
    "    m = y.size\n",
    "    J = 0\n",
    "    Theta1_grad = np.zeros(Theta1.shape)\n",
    "    Theta2_grad = np.zeros(Theta2.shape)\n",
    "    # Convert labels to one-hot encoding\n",
    "    y_onehot = np.zeros((m, num_labels))\n",
    "    y_onehot[np.arange(m), y] = 1\n",
    "    # Forward propagation\n",
    "    a1 = np.insert(X, 0, 1, axis=1)  # Add bias unit to input layer\n",
    "    z2 = a1.dot(Theta1.T)\n",
    "    a2 = utils.sigmoid(z2)\n",
    "    a2 = np.insert(a2, 0, 1, axis=1)  # Add bias unit to hidden layer\n",
    "    z3 = a2.dot(Theta2.T)\n",
    "    a3 = utils.sigmoid(z3)  # Output of the neural network\n",
    "    # Compute the cost without regularization\n",
    "    J = (-1 / m) * np.sum(y_onehot * np.log(a3) + (1 - y_onehot) * np.log(1 - a3))\n",
    "    # Compute regularization term (excluding bias terms)\n",
    "    reg_term = (lambda_ / (2 * m)) * (\n",
    "        np.sum(Theta1[:, 1:] ** 2) + np.sum(Theta2[:, 1:] ** 2)\n",
    "    )\n",
    "    J = J + reg_term  # Add regularization term to the cost\n",
    "    # Compute the gradients using backpropagation\n",
    "    delta3 = a3 - y_onehot\n",
    "    delta2 = delta3.dot(Theta2) * a2 * (1 - a2)\n",
    "    delta2 = delta2[:, 1:]  # Remove delta for the bias unit\n",
    "    Theta1_grad = (1 / m) * delta2.T.dot(a1)\n",
    "    Theta2_grad = (1 / m) * delta3.T.dot(a2)\n",
    "    # Add regularization to the gradients (except for the bias terms)\n",
    "    Theta1_grad[:, 1:] += (lambda_ / m) * Theta1[:, 1:]\n",
    "    Theta2_grad[:, 1:] += (lambda_ / m) * Theta2[:, 1:]\n",
    "    # Unroll gradients\n",
    "    grad = np.concatenate([Theta1_grad.ravel(), Theta2_grad.ravel()])\n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWoYCNYIYwdo"
   },
   "source": [
    "Once finished, we will call the `nnCostFunction` function using the parameters loaded for `Theta1` and `Theta2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 478,
     "status": "ok",
     "timestamp": 1697118238581,
     "user": {
      "displayName": "Arthur C.",
      "userId": "18023806256318143056"
     },
     "user_tz": 180
    },
    "id": "8ePaNRneYwdo",
    "outputId": "c093b4c7-9520-4614-f8ff-d652ce0760b1"
   },
   "outputs": [],
   "source": [
    "lambda_ = 0\n",
    "J, _ = nnCostFunction(\n",
    "    nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lambda_\n",
    ")\n",
    "print(\"Cost at parameters (loaded from p3bweights): %.6f \" % J)\n",
    "print(\"The cost should be about                   : 0.287629.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHlP-yrdYwds"
   },
   "source": [
    "Once finished, the next cell will call `nnCostFunction` using the parameters loaded from `Theta1` and `Theta2`, and $\\lambda = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1697118238582,
     "user": {
      "displayName": "Arthur C.",
      "userId": "18023806256318143056"
     },
     "user_tz": 180
    },
    "id": "-vM6TpzOYwds",
    "outputId": "405f1d12-6edc-4c70-82bf-e4caf3bb531e"
   },
   "outputs": [],
   "source": [
    "# Weight regularization parameter (we set this to 1 here).\n",
    "lambda_ = 1\n",
    "J, _ = nnCostFunction(\n",
    "    nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lambda_\n",
    ")\n",
    "\n",
    "print(\"Cost for the parameters (loaded from p3bweights): %.6f\" % J)\n",
    "print(\"The cost should be about                        : 0.383770.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQqYkYmzYwdy"
   },
   "source": [
    "## Backpropagation\n",
    "\n",
    "In this section of the project, we will implement the *backpropagation* algorithm to calculate the gradient of the neural network's cost function. We will update the `nnCostFunction` function which will then return an appropriate value for `grad`. Once the gradient has been obtained, we will be able to train the neural network to minimize the cost function $J(\\theta)$ using an advanced optimizer.\n",
    "\n",
    "We will first implement the *backpropagation* algorithm to calculate the gradient for the parameters of the (unregularized) neural network. After checking that the gradient calculation for the unregularized case is correct, we will implement the gradient for the regularized neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXL8Iij2Ywdy"
   },
   "source": [
    "### Sigmoid gradient\n",
    "First, we will implement the gradient of the sigmoid function. The gradient for the sigmoid function can be calculated as\n",
    "\n",
    "$$ g'(z) = \\frac{d}{dz} g(z) = g(z)\\left(1-g(z)\\right) $$\n",
    "where\n",
    "$$ \\text{sigmoid}(z) = g(z) = \\frac{1}{1 + e^{-z}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1697118238582,
     "user": {
      "displayName": "Arthur C.",
      "userId": "18023806256318143056"
     },
     "user_tz": 180
    },
    "id": "Ykiwq9YtYwdz"
   },
   "outputs": [],
   "source": [
    "def sigmoidGradient(z):\n",
    "    \"\"\"\n",
    "    Computes the gradient of the sigmoid function evaluated at z.\n",
    "    This should work regardless if z is a matrix or a vector.\n",
    "    In particular, if z is a vector or matrix, you should return\n",
    "    the gradient for each element.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    z : array_like\n",
    "        A vector or matrix as input to the sigmoid function.\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    g : array_like\n",
    "        Gradient of the sigmoid function. Has the same shape as z.\n",
    "    \"\"\"\n",
    "    g = np.zeros(z.shape)\n",
    "    sigmoid_z = utils.sigmoid(z)\n",
    "    g = sigmoid_z * (1 - sigmoid_z)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8M3L7Gs5Ywd1"
   },
   "source": [
    "Once this is done, the next cell will call `sigmoidGradient` in the `z` vector. When $z = 0$, the gradient must be exactly 0.25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1697118238582,
     "user": {
      "displayName": "Arthur C.",
      "userId": "18023806256318143056"
     },
     "user_tz": 180
    },
    "id": "V3eyvFViYwd1",
    "outputId": "9ccf7042-4be0-4b27-90d8-417444b8b86a"
   },
   "outputs": [],
   "source": [
    "z = np.array([-1, -0.5, 0, 0.5, 1])\n",
    "g = sigmoidGradient(z)\n",
    "print(\"Sigmoid gradient evaluated at [-1 -0.5 0 0.5 1]:\")\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "szXW1gA-Ywd3"
   },
   "source": [
    "## Random initialization\n",
    "\n",
    "When training neural networks, it is important to randomize the initialization of the parameters to break the symmetry. An effective strategy for randomizing the initialization is to randomize the selected values of $\\Theta^{(l)}$ uniformly in the range $[-\\epsilon_{init}, \\epsilon_{init}]$. We will use $\\epsilon_{init} = 0.12$. This range of values ensures that the parameters are kept low, which makes learning more efficient.\n",
    "\n",
    "An effective strategy for choosing $\\epsilon_{init}$ is to base it on the number of units in the network. A good choice of $\\epsilon_{init}$ is $\\epsilon_{init} = \\frac{\\sqrt{6}}{\\sqrt{L_{in} + L_{out}}}$ where $L_{in} = s_l$ and $L_{out} = s_{l+1}$ is the number of units in the adjacent layers for $\\Theta^{l}$.\n",
    "Next, the `randInitializeWeights` function will be implemented to initialize the weights for $\\Theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1697118238583,
     "user": {
      "displayName": "Arthur C.",
      "userId": "18023806256318143056"
     },
     "user_tz": 180
    },
    "id": "m7SE8WO6Ywd4"
   },
   "outputs": [],
   "source": [
    "def randInitializeWeights(L_in, L_out, epsilon_init=0.12):\n",
    "    \"\"\"\n",
    "    Randomly initialize the weights of a layer in a neural network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    L_in : int\n",
    "        Number of incomming connections.\n",
    "\n",
    "    L_out : int\n",
    "        Number of outgoing connections.\n",
    "\n",
    "    epsilon_init : float, optional\n",
    "        Range of values which the weight can take from a uniform\n",
    "        distribution.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    W : array_like\n",
    "        The weight initialiatized to random values.  Note that W should\n",
    "        be set to a matrix of size(L_out, 1 + L_in) as\n",
    "        the first column of W handles the \"bias\" terms.\n",
    "    \"\"\"\n",
    "    W = np.zeros((L_out, 1 + L_in))\n",
    "    W = np.random.rand(L_out, 1 + L_in) * 2 * epsilon_init - epsilon_init\n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8kbaw4rYwd6"
   },
   "source": [
    "We will run the next cell to initialize the weights for the two layers of the neural network using the `randInitializeWeights` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1697118238583,
     "user": {
      "displayName": "Arthur C.",
      "userId": "18023806256318143056"
     },
     "user_tz": 180
    },
    "id": "11CeDzXDYwd6",
    "outputId": "108b9cd3-04bf-4f6b-be7d-a58043c9465e"
   },
   "outputs": [],
   "source": [
    "print(\"Initializing the neural network parameters...\")\n",
    "\n",
    "initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size)\n",
    "initial_Theta2 = randInitializeWeights(hidden_layer_size, num_labels)\n",
    "\n",
    "# Unroll parameters\n",
    "initial_nn_params = np.concatenate(\n",
    "    [initial_Theta1.ravel(), initial_Theta2.ravel()], axis=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TE2TRaA9Ywd9"
   },
   "source": [
    "### Backpropagation\n",
    "\n",
    "![](figures/ex4-backpropagation.png)\n",
    "\n",
    "Now we're going to implement the *backpropagation* algorithm. Recall that the intuition behind the *backpropagation* algorithm is as follows. Given a training example $(x^{(t)}, y^{(t)})$, we will first perform a*feedforward pass* to calculate all the activations along the network, including the output value of the hypothesis $h_\\theta(x)$. So, for each node $j$ in layer $l$, we want to calculate the “error term” $\\delta_j^{(l)}$ which measures how much the node is “responsible” for any error in our output.\n",
    "\n",
    "For an output node, we can directly measure the difference between the network activation and the true target value and use this to define $\\delta_j^{(3)}$ (since layer 3 is the output layer). For the hidden units, you will calculate $\\delta_j^{(l)}$ based on the weighted average of the error terms of the nodes in layer $(l+1)$. You should implement steps 1 to 4 in a loop that processes one example at a time. Concretely, you should implement a for `for t in range(m)` loop and place steps 1 to 4 below inside the loop, with the $t^{th}$ iteration performing the calculation on the $t^{th}$ training example $(x^{(t)},y^{(t)})$. Step 5 will divide the accumulated gradients by $m$ to obtain the gradients for the neural network's cost function.\n",
    "\n",
    "1. Select the input layer values $(a^{(1)})$ for the $t^{th }$ training examples $x^{(t)}$. Perform a direct pass, calculating the activations $(z^{(2)}, a^{(2)}, z^{(3)}, a^{(3)})$ for layers 2 and 3. Note that you need to add a `+1` term to make sure that the activation vectors for layers $a^{(1)}$ and $a^{(2)}$ also include the unit *bias*. In `numpy`, if `a_1` is a column matrix, adding a column of `1` corresponds to `a_1 = np.concatenate([np.ones((m, 1)), a_1], axis=1)`.\n",
    "\n",
    "1. For each output unit $k$ in layer 3 (the output layer), define\n",
    "$$\\delta_k^{(3)} = \\left (a_k^{(3)} - y_k \\right)$$\n",
    "where $ y_k \\in \\{0, 1\\} $ indicates whether the current training example belongs to class $k$ $(y_k = 1)$ or whether it belongs to a different class $ (y_k = 0)$. You can find useful logical matrices for this task (explained in Project 3A).\n",
    "\n",
    "1. For the hidden layer $ l = 2 $, define\n",
    "$$ \\delta^{(2)} = \\left (\\Theta^{(2)} \\right)^T \\delta^{(3)} * g'\\left(z^{(2)} \\right)$$\n",
    "Note that the symbol $ * $ performs multiplication by elements in `numpy`.\n",
    "\n",
    "1. Accumulate the gradient of this example using the following formula. Note that you must skip or remove $ \\delta_0^{(2)}$. In `numpy`, removing $ \\delta_0^{(2)}$ corresponds to `delta_2 = delta_2[1:] `.\n",
    "$$ \\Delta^{(l)} = \\Delta^{(l)} + \\delta^{(l + 1)} (a^{(l)})^{(T)}$$\n",
    "\n",
    "1. Obtain the (unregularized) gradient for the cost function of the neural network by dividing the accumulated gradients by $ \\frac{1} {m}$:\n",
    "$$ \\frac{\\partial}{\\partial \\Theta_{ij}^{(l)}} J(\\Theta) = D_{ij}^{(l)} = \\frac{1}{m} \\Delta_{ij}^{(l)}$$\n",
    "\n",
    "Go back and check the `nnCostFunction` with the *backpropagation* algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LnglIRyUYwd9"
   },
   "source": [
    "After implementing the *backpropagation* algorithm, we will perform gradient checking on your implementation. Gradient checking will increase confidence that your code is calculating gradients correctly.\n",
    "\n",
    "### Gradient checking  \n",
    "\n",
    "In our neural network, we are minimizing the cost function $J(\\Theta)$. To use gradient checking on our parameters, we can imagine that we are “unrolling” the parameters $\\Theta^{(1)}$ and $\\Theta^{(2)}$ into a single vector $\\theta$. In doing so, we can think of the cost function as $J(\\Theta)$ and use the following gradient checking procedure.\n",
    "Suppose we have the function $f_i(\\theta)$ which is supposed to calculate $\\frac{\\partial}{\\partial \\theta_i} J(\\theta)$; we would like to check that $f_i$ is producing correct derivative values.\n",
    "\n",
    "$$\n",
    "\\text{Seja } \\theta^{(i+)} = \\theta + \\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ \\epsilon \\\\ \\vdots \\\\ 0 \\end{bmatrix}\n",
    "\\quad \\text{e} \\quad \\theta^{(i-)} = \\theta - \\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ \\epsilon \\\\ \\vdots \\\\ 0 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "So, $\\theta^{(i+)}$ is equal to $\\theta$, except for the $i^{th}$ element, which has been added by $\\epsilon$. Similarly, $\\theta^{(i-)}$ is equal to the vector $\\theta$, except for the $i^{th}$ element, which has been decreased by $\\epsilon$. We can now numerically check the correctness of the $f_i(\\theta)$'s by checking for each $i$:\n",
    "\n",
    "$$ f_i\\left( \\theta \\right) \\approx \\frac{J\\left( \\theta^{(i+)}\\right) - J\\left( \\theta^{(i-)} \\right)}{2\\epsilon} $$\n",
    "\n",
    "How close these two values are depends on $J$. But, assuming $ \\epsilon = 10 ^ {- 4} $, we will generally find that the left and right sides must agree on at least the 4 most significant digits.\n",
    "\n",
    "In the next cell we will run the supplied function `checkNNGradients` with which we will create a small neural network and the dataset that will be used to check our gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1697118238583,
     "user": {
      "displayName": "Arthur C.",
      "userId": "18023806256318143056"
     },
     "user_tz": 180
    },
    "id": "4bpwvzB-Ywd9",
    "outputId": "5fe55b6c-be91-4217-abe8-7ca3c7d4f412"
   },
   "outputs": [],
   "source": [
    "utils.checkNNGradients(nnCostFunction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p0m_bhGsYwd_"
   },
   "source": [
    "### Regularized Neural Network\n",
    "\n",
    "After implementing the *backpropagation* algorithm correctly, gradient regularization was added. To take regularization into account, it was enough to add an additional term after calculating the gradients using the *backpropagation* algorithm.\n",
    "\n",
    "Specifically, after calculating $\\Delta_{ij}^{(l)}$ using the *backpropagation* algorithm, we added the regularization using\n",
    "\n",
    "$$ \\begin{align}\n",
    "& \\frac{\\partial}{\\partial \\Theta_{ij}^{(l)}} J(\\Theta) = D_{ij}^{(l)} = \\frac{1}{m} \\Delta_{ij}^{(l)} & \\qquad \\text{for } j = 0 \\\\\n",
    "& \\frac{\\partial}{\\partial \\Theta_{ij}^{(l)}} J(\\Theta) = D_{ij}^{(l)} = \\frac{1}{m} \\Delta_{ij}^{(l)} + \\frac{\\lambda}{m} \\Theta_{ij}^{(l)} & \\qquad \\text{for } j \\ge 1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note that we should NOT regularize the first column of $\\Theta^{(l)}$, since it is used to accommodate bias terms. Furthermore, in the parameters $\\Theta_{ij}^{(l)}$, $i$ is indexed starting at 1 and $j$ is indexed starting at 0. Thus,\n",
    "\n",
    "$$\n",
    "\\Theta^{(l)} = \\begin{bmatrix}\n",
    "\\Theta_{1,0}^{(i)} & \\Theta_{1,1}^{(l)} & \\cdots \\\\\n",
    "\\Theta_{2,0}^{(i)} & \\Theta_{2,1}^{(l)} & \\cdots \\\\\n",
    "\\vdots & ~ & \\ddots\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The next cell will perform the gradient check in our implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1697118238583,
     "user": {
      "displayName": "Arthur C.",
      "userId": "18023806256318143056"
     },
     "user_tz": 180
    },
    "id": "nEF77d4lYweA",
    "outputId": "09ffbeca-043a-40ae-d170-9f178cf8ded4"
   },
   "outputs": [],
   "source": [
    "#  Check gradients by running checkNNGradients\n",
    "lambda_ = 3\n",
    "utils.checkNNGradients(nnCostFunction, lambda_)\n",
    "\n",
    "# Also output the costFunction debugging values\n",
    "debug_J, _ = nnCostFunction(\n",
    "    nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lambda_\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"\\n\\nCost at (fixed) debugging parameters (w/ lambda = %f): %f \"\n",
    "    % (lambda_, debug_J)\n",
    ")\n",
    "print(\"(for lambda = 3, this value should be about 0.576051)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d65eD2PRYweB"
   },
   "source": [
    "### Parameter learning using `scipy.optimize.minimize`\n",
    "\n",
    "After implementing the neural network's cost function and calculating the gradient, the next step is to use `scipy` minimization to learn good parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15029,
     "status": "ok",
     "timestamp": 1697118253603,
     "user": {
      "displayName": "Arthur C.",
      "userId": "18023806256318143056"
     },
     "user_tz": 180
    },
    "id": "rxPyeIieYweC",
    "outputId": "c8ac6dae-33e5-49dd-c148-90dc16af5560"
   },
   "outputs": [],
   "source": [
    "#  After you have completed the assignment, change the maxiter to a larger\n",
    "#  value to see how more training helps.\n",
    "options = {\"maxiter\": 100}\n",
    "\n",
    "#  You should also try different values of lambda\n",
    "lambda_ = 1\n",
    "\n",
    "# Create \"short hand\" for the cost function to be minimized\n",
    "costFunction = lambda p: nnCostFunction(\n",
    "    p, input_layer_size, hidden_layer_size, num_labels, X, y, lambda_\n",
    ")\n",
    "\n",
    "# Now, costFunction is a function that takes in only one argument\n",
    "# (the neural network parameters)\n",
    "res = optimize.minimize(\n",
    "    costFunction, initial_nn_params, jac=True, method=\"TNC\", options=options\n",
    ")\n",
    "\n",
    "# get the solution of the optimization\n",
    "nn_params = res.x\n",
    "\n",
    "# Obtain Theta1 and Theta2 back from nn_params\n",
    "Theta1 = np.reshape(\n",
    "    nn_params[: hidden_layer_size * (input_layer_size + 1)],\n",
    "    (hidden_layer_size, (input_layer_size + 1)),\n",
    ")\n",
    "\n",
    "Theta2 = np.reshape(\n",
    "    nn_params[(hidden_layer_size * (input_layer_size + 1)) :],\n",
    "    (num_labels, (hidden_layer_size + 1)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aUdqYuNmYweE"
   },
   "source": [
    "After completing the training, we will calculate the training accuracy of our classifier by calculating the percentage of correct examples. If our implementation is correct, we will obtain a training accuracy of around 95.3% (this can vary by approximately 1% due to random initialization). It is possible to obtain higher accuracies by training the neural network in more iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1697118253603,
     "user": {
      "displayName": "Arthur C.",
      "userId": "18023806256318143056"
     },
     "user_tz": 180
    },
    "id": "7zejfC0GYweE",
    "outputId": "703cbf59-61ab-458f-9734-fce38c9e79ef"
   },
   "outputs": [],
   "source": [
    "pred = utils.predict(Theta1, Theta2, X)\n",
    "print(\"Training accuracy: %f\" % (np.mean(pred == y) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x4wxrviFYweG"
   },
   "source": [
    "## Visualizing the Hidden Layer\n",
    "\n",
    "One way of understanding what the neural network is learning is to visualize what representations are captured by the hidden layer units. Informally, given a particular hidden layer unit, one way to visualize what is being calculated is to find an input $x$ that will cause it to activate (i.e. have an activation value ($ a_i^{(l)}$) close to 1). For the neural network we trained, note that the $i^{th}$ row of $\\Theta^{(1)}$ is a vector of size 401 that represents the parameter for the $i^{th}$ unit of the hidden layer. If we discard the *bias* term, we get a vector of size 400 representing the weights of each input pixel of the hidden layer.\n",
    "\n",
    "So, one way to visualize the “representation” captured by the hidden layer unit is to reshape this vector of size 400 into a 20x20 image and display it (this is equivalent to finding the input that offers the greatest activation for the hidden unit, given a “norm” constraint on the input (i.e. $ ||x|_2 \\le 1 $)).\n",
    "\n",
    "The next cell does this using the `displayData` function and this is shown in an image with 25 units, each corresponding to a unit of the network's hidden layer. In our trained network, you should find that the hidden layer units roughly correspond to the detectors that look for traces and other patterns in the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 807
    },
    "executionInfo": {
     "elapsed": 1024,
     "status": "ok",
     "timestamp": 1697118254625,
     "user": {
      "displayName": "Arthur C.",
      "userId": "18023806256318143056"
     },
     "user_tz": 180
    },
    "id": "qSqYuG47YweG",
    "outputId": "e79c0454-38e1-4415-a633-c3ac8f968e1a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "utils.displayData(Theta1[:, 1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "frALLAeAYweI"
   },
   "source": [
    "# Comparison with PyTorch\n",
    "\n",
    "Now, we will build the same neural network for classification presented in the previous sections, but instead of building the functions that execute each step of the neural network, we will use the functions available in the Pytorch library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1697118254626,
     "user": {
      "displayName": "Arthur C.",
      "userId": "18023806256318143056"
     },
     "user_tz": 180
    },
    "id": "I3a57H_rYweI"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-psdgbAeYweL"
   },
   "source": [
    "### Creating the model\n",
    "\n",
    "The structure of our neural network will be the same as that presented in subsection 1.2, in which we have an input layer, a hidden layer and an output layer. We will now build this network using Pytorch. Recalling the structure, we have\n",
    "\n",
    "- An input layer with Linear activation function with:\n",
    "\n",
    "        -> Input: 400 \n",
    "        -> Output: 25 \n",
    "        -> Bias: True \n",
    "    \n",
    "- A hidden layer with Sigmoid activation function with:\n",
    "\n",
    "        -> Input: 25  \n",
    "        -> Output: 10 \n",
    "        -> Bias: True \n",
    "\n",
    "- And the output layer containing the outputs with the 10 expected classes (one for each digit).\n",
    "\n",
    "So, we'll build our neural network from the NeuralNetwork class. This class is initialized by defining the `fc1` and `fc2` functions, which perform linear transformations on the data in each layer, the former in the input layer (with 400 inputs and 25 outputs) and the latter in the hidden layer (with 25 inputs and 10 outputs).\n",
    "Next, we'll define the forward() function which executes the Sigmoid function as the activation functions for the two layers in question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1697118254626,
     "user": {
      "displayName": "Arthur C.",
      "userId": "18023806256318143056"
     },
     "user_tz": 180
    },
    "id": "5a1bcbEkYweL"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(in_features=400, out_features=25, bias=True)\n",
    "        self.fc2 = torch.nn.Linear(in_features=25, out_features=10, bias=True)\n",
    "\n",
    "    def forward(self, out):\n",
    "        out = torch.sigmoid(self.fc1(out))\n",
    "        out = torch.sigmoid(self.fc2(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9Ylg5Z6YweN"
   },
   "source": [
    "### Processing and handling the input data\n",
    "\n",
    "Once the structure of the neural network has been created, we need to process the input data, adapting it to a format in which the class we have created can process it in order to learn the network's parameters. First, we'll re-import the project data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1697118254626,
     "user": {
      "displayName": "Arthur C.",
      "userId": "18023806256318143056"
     },
     "user_tz": 180
    },
    "id": "EDPXxn0ZYweN"
   },
   "outputs": [],
   "source": [
    "# Import the data\n",
    "datas = loadmat(\"data/p3bdata1.mat\")\n",
    "datas.keys()\n",
    "\n",
    "label, images = datas[\"y\"], datas[\"X\"]\n",
    "# This is an artifact due to the fact that this dataset was used in\n",
    "# MATLAB where there is no index 0\n",
    "label = label - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LvqG3HaT6mri"
   },
   "source": [
    "With the data imported and basically adjusted, we will transform it into tensors before creating the dataset. To do this, we'll use the `torch.Tensor()` and `torch.LongTensor()` methods. The difference between the two is that the former accepts values of type `float` (more specifically `float32`) while the latter accepts values of type `int` (more specifically `int64`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1697118254627,
     "user": {
      "displayName": "Arthur C.",
      "userId": "18023806256318143056"
     },
     "user_tz": 180
    },
    "id": "QHLi23dS-w6r"
   },
   "outputs": [],
   "source": [
    "# Transform the inputs and labels into tensors\n",
    "tensor_images, tensor_label = torch.Tensor(images), torch.LongTensor(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5fbSFslN-29-"
   },
   "source": [
    "Once this is done, we can create our dataset which will be used to train the neural network. To do this, we will use the `TensorDataset` and `DataLoader` functions.\n",
    "\n",
    "In these functions, we will select the desired size for the *batch*, as well as the option to*shuffle* the data to improve the performance of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1697118254627,
     "user": {
      "displayName": "Arthur C.",
      "userId": "18023806256318143056"
     },
     "user_tz": 180
    },
    "id": "BAcFivtkYweQ"
   },
   "outputs": [],
   "source": [
    "# Batch Size\n",
    "batch_size = 35\n",
    "\n",
    "# Creating the dataset with the two tensors\n",
    "dataset = TensorDataset(tensor_images, tensor_label)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGeV0GIpYweT"
   },
   "source": [
    "### Training the Neural Network\n",
    "\n",
    "To train the neural network, it is necessary to choose which cost function will be used and which algorithm will optimize the network parameters.\n",
    "\n",
    "PyTorch provides several types of cost function within the `torch.nn` module. Among the best known are *Cross Entropy* and *mean squared error*. As for the optimization algorithm, the library also provides several types, among the best known are the *Gradient Descent*, the *Stochastic Gradient Descent* and Adam. In our program, we chose *Cross Entropy* for the cost function and, as the optimization algorithm, we chose *Stochastic Gradient Descent* with initial values of *learning rate* of 0.5 and *momentum* of 0.9.\n",
    "\n",
    "With everything set up, we can instantiate our neural network in the `model` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1697118254628,
     "user": {
      "displayName": "Arthur C.",
      "userId": "18023806256318143056"
     },
     "user_tz": 180
    },
    "id": "65H5EW_AYweU"
   },
   "outputs": [],
   "source": [
    "# Instantiate the NN into variable model\n",
    "model = NeuralNetwork()\n",
    "\n",
    "# Definition of cost function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Definition of SGD as optimizer algorithm and its parameter sets\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.5, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vDj90WJLdiN"
   },
   "source": [
    "We will print the `model` variable so that we can see its details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1697118254628,
     "user": {
      "displayName": "Arthur C.",
      "userId": "18023806256318143056"
     },
     "user_tz": 180
    },
    "id": "IKG5gCMfYweW",
    "outputId": "1b4c49ab-a4e9-41a4-d0f7-37a3ced5fa73"
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ozIxOJgxYweY"
   },
   "source": [
    "Once the training data has been processed correctly and the model has been properly instantiated with the cost function and optimization algorithm configured, we must build the network's training loop.\n",
    "\n",
    "We will first choose the number of epochs that our neural network will go through in this training stage. In each of these epochs, the neural network must create another loop in which each example of the dataloader will be explored, calculating the error and optimizing the parameters at the end of each of these examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1697118254628,
     "user": {
      "displayName": "Arthur C.",
      "userId": "18023806256318143056"
     },
     "user_tz": 180
    },
    "id": "3en2wjIFYweY"
   },
   "outputs": [],
   "source": [
    "def training(NN, dataloader, criterion, lr=0.5):\n",
    "    \"\"\"\n",
    "    Train the neural network to improve the predictions and, consequently,\n",
    "    the accuracy os the model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    NN : class_like\n",
    "        A class that contains the structure of neural network.\n",
    "\n",
    "    dataloader: class_like\n",
    "        A class that contains the training data set with images and labels.\n",
    "\n",
    "    criterion: class_like\n",
    "        A class that contains the cost function.\n",
    "\n",
    "    lr: float_like\n",
    "        The number of the learning rate. By default, it is equal to 0.5.\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    costs : list_like\n",
    "        List of cost function values ​​for each epoch.\n",
    "\n",
    "    accs : list_like\n",
    "        List of accuracy values for each epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    # Recreates the model and the optimizer to ensure that we will not train an already trained model.\n",
    "    model = NN\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "    # Define the number os epochs.\n",
    "    num_epochs = 100\n",
    "\n",
    "    # Creation of variables to save train data\n",
    "    cost = torch.zeros([])\n",
    "    output = torch.zeros(batch_size, 10)\n",
    "    costs = []\n",
    "    accs = []\n",
    "\n",
    "    # Loop that will run the code num_epoch times\n",
    "    for epoch in range(num_epochs):\n",
    "        # Variables to calculate accuracy\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # Indicate that model is on train mode\n",
    "        model.train()\n",
    "        for data, label in dataloader:\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            output = model(data)\n",
    "            label = label.view(-1)\n",
    "            # Compute the loss\n",
    "            loss = criterion(output, label)\n",
    "            # Backpropagation and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Append the current batch loss to the `costs` list\n",
    "            cost += loss.item()\n",
    "            # Counting of correct labels and total examples\n",
    "            out2 = torch.log_softmax(output, dim=1)\n",
    "            _, predict = torch.max(out2, dim=1)\n",
    "            for i in range(0, len(label)):\n",
    "                total = total + 1\n",
    "                if predict[i] == label[i]:\n",
    "                    correct = correct + 1\n",
    "\n",
    "        # Computation of accuracy\n",
    "        acc = (\n",
    "            correct / total\n",
    "        ) * 100  # Calcula a acurácia para erro absoluto de 1 grau em porcentagem\n",
    "\n",
    "        # Adding elements to lists\n",
    "        accs.append(acc)  # Adiciona essa acurácia a sua lista\n",
    "        costs.append(cost.item())\n",
    "\n",
    "        print(\"epoch [{}/{}], cost:{:.4f}\".format(epoch + 1, num_epochs, cost.item()))\n",
    "\n",
    "    return costs, accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6BqiTl-6mcj"
   },
   "source": [
    "In the `training` function we run the neural network training and, within the loop, we have already collected the values of the cost function and accuracy at each epoch, storing them in two lists that are returned at the end of our function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27303,
     "status": "ok",
     "timestamp": 1697118281917,
     "user": {
      "displayName": "Arthur C.",
      "userId": "18023806256318143056"
     },
     "user_tz": 180
    },
    "id": "fh5E_MMTStxY",
    "outputId": "c955c07e-4dca-4d2b-8ce9-967fca495c0a"
   },
   "outputs": [],
   "source": [
    "costs, accs = training(NN=NeuralNetwork(), dataloader=dataloader, criterion=criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFsrpPdi-VF_"
   },
   "source": [
    "With the network trained, we can plot the values of the cost function as a function of the number of epochs, so that we can analyze its behavior and see if it has converged as expected. The cell below plots the two graphs (of the loss function and accuracy). It is expected that, at the end of the 100 epochs, the loss function will be between 1.45 and 1.55 and the accuracy will be above 95%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 886,
     "status": "ok",
     "timestamp": 1697118282783,
     "user": {
      "displayName": "Arthur C.",
      "userId": "18023806256318143056"
     },
     "user_tz": 180
    },
    "id": "nt_kcgNigKhi",
    "outputId": "0e3f4b71-618b-4d20-c510-52cdac0bba90"
   },
   "outputs": [],
   "source": [
    "pyplot.figure(figsize=(15, 8))\n",
    "pyplot.plot(costs)\n",
    "pyplot.title(\"Cost x Epochs\")\n",
    "pyplot.xlabel(\"Epochs\")\n",
    "pyplot.ylabel(\"Cost\")\n",
    "pyplot.grid(True)\n",
    "pyplot.show()\n",
    "\n",
    "pyplot.figure(figsize=(15, 8))\n",
    "pyplot.plot(accs)\n",
    "pyplot.yticks(range(0, 101, 5))\n",
    "pyplot.title(\"Accuracy x epochs\")\n",
    "pyplot.xlabel(\"Epochs\")\n",
    "pyplot.ylabel(\"Accuracy (%)\")\n",
    "pyplot.grid(True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yf8UHPc0GXoV"
   },
   "source": [
    "We will now see how these two curves behave when we change the value of the *learning rate*. We will now train the network with different `lr` values: `lr=0.005`, `lr=0.015`, `lr=15` and `lr=50`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 109103,
     "status": "ok",
     "timestamp": 1697118391873,
     "user": {
      "displayName": "Arthur C.",
      "userId": "18023806256318143056"
     },
     "user_tz": 180
    },
    "id": "KWbY_GFBEg7u",
    "outputId": "8b1a5259-5c33-4baf-d03c-48dd6ac823f2"
   },
   "outputs": [],
   "source": [
    "costs_lr0005, accs_lr0005 = training(\n",
    "    NN=NeuralNetwork(), dataloader=dataloader, criterion=criterion, lr=0.005\n",
    ")\n",
    "costs_lr0015, accs_lr0015 = training(\n",
    "    NN=NeuralNetwork(), dataloader=dataloader, criterion=criterion, lr=0.015\n",
    ")\n",
    "costs_lr15, accs_lr15 = training(\n",
    "    NN=NeuralNetwork(), dataloader=dataloader, criterion=criterion, lr=15\n",
    ")\n",
    "costs_lr50, accs_lr50 = training(\n",
    "    NN=NeuralNetwork(), dataloader=dataloader, criterion=criterion, lr=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1317,
     "status": "ok",
     "timestamp": 1697118393184,
     "user": {
      "displayName": "Arthur C.",
      "userId": "18023806256318143056"
     },
     "user_tz": 180
    },
    "id": "nCM5UbmeFB5H",
    "outputId": "f61df644-3787-47cc-ec07-c20ae384eb57"
   },
   "outputs": [],
   "source": [
    "pyplot.figure(figsize=(15, 8))\n",
    "pyplot.plot(costs_lr0005, label=\"Cost to lr=0.005\")\n",
    "pyplot.plot(costs_lr0015, label=\"Cost to lr=0.015\")\n",
    "pyplot.plot(costs_lr15, label=\"Cost to lr=15\")\n",
    "pyplot.plot(costs_lr50, label=\"Cost to lr=50\")\n",
    "pyplot.title(\"Costs x Epochs\")\n",
    "pyplot.xlabel(\"Epochs\")\n",
    "pyplot.ylabel(\"Costs\")\n",
    "pyplot.legend()\n",
    "pyplot.grid(True)\n",
    "pyplot.show()\n",
    "\n",
    "pyplot.figure(figsize=(15, 8))\n",
    "pyplot.plot(accs_lr0005, label=\"Accuracy to lr=0.005\")\n",
    "pyplot.plot(accs_lr0015, label=\"Accuracy to lr=0.015\")\n",
    "pyplot.plot(accs_lr15, label=\"Accuracy to lr=15\")\n",
    "pyplot.plot(accs_lr50, label=\"Accuracy to lr=50\")\n",
    "pyplot.yticks(range(0, 101, 5))\n",
    "pyplot.title(\"Accuracy x epochs\")\n",
    "pyplot.xlabel(\"Epochs\")\n",
    "pyplot.ylabel(\"Accuracy (%)\")\n",
    "pyplot.legend(loc=\"lower right\")\n",
    "pyplot.grid(True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6EaiTkKrH1dv"
   },
   "source": [
    "See how the algorithm in the two cases where `lr` is very small converges, but slowly, so that after 100 epochs, the accuracy still hasn't exceeded 95%.\n",
    "\n",
    "For `lr=15` the algorithm converges, but looking at the shape of the two curves (cost and accuracy), it seems to be on the threshold between convergence and divergence. Although its loss function remains at an acceptable value at the end of the 100 epochs, the accuracy makes it clear that its performance is worse than for `lr=0.5`, since it does not exceed 90%.  \n",
    "\n",
    "On the other hand, for `lr=50` the algorithm does not converge. It is interesting to note the behavior of the accuracy curve of this model. With divergence, the algorithm is unable to improve its parameters and so it is as if it were trying to guess the labels at random. In this case, as the data is divided into 10 equiprobable classes, it is expected that the accuracy value will be around 10%, and this is exactly what happens during all the seasons."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
