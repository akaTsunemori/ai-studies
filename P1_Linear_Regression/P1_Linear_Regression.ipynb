{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNzuIYYhnPtJ"
   },
   "source": [
    "# Project 1: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1694527359943,
     "user": {
      "displayName": "Arthur C.",
      "userId": "18023806256318143056"
     },
     "user_tz": 180
    },
    "id": "GpARh0oMqK3E"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oi7GUmF4qK3g"
   },
   "source": [
    "## Linear regression of one variable\n",
    "\n",
    "We're going to implement linear regression for one variable to try to predict the profits of a food truck. Suppose you are the CEO of a food truck franchise and you are considering different cities to open a new branch. The franchise already has other trucks in several cities and you have the profitability data for each of them, as well as the population of each of these municipalities. You plan to use this data to choose which city to open the next truck.\n",
    "\n",
    "The document `data/data1.txt` contains a dataset for our regression problem. The first column is the population of the city (in 10,000 inhabitants) and the second column is the profit of the food truck in this city (in 10,000 dollars). A negative value for profit means a loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1694527359947,
     "user": {
      "displayName": "Arthur C.",
      "userId": "18023806256318143056"
     },
     "user_tz": 180
    },
    "id": "bbcaugE1qK3h"
   },
   "outputs": [],
   "source": [
    "# Read comma separated data\n",
    "data = np.loadtxt(os.path.join(\"data/\", \"p1data1.txt\"), delimiter=\",\")\n",
    "X, y = data[:, 0], data[:, 1]\n",
    "m = y.size  # Number of training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y5KDMqBWqK3n"
   },
   "source": [
    "### Plotting the data\n",
    "\n",
    "Before starting any task, it is interesting to visualize the data. For this dataset, we can use a scatter plot to visualize the data, since we only have one feature (population)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1694527359948,
     "user": {
      "displayName": "Arthur C.",
      "userId": "18023806256318143056"
     },
     "user_tz": 180
    },
    "id": "ejPi1G2YqK3p"
   },
   "outputs": [],
   "source": [
    "def plotData(X, y):\n",
    "    \"\"\"\n",
    "    Plots the data points x and y into a new figure. Plots the data\n",
    "    points and gives the figure axes labels of population and profit.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        Data point values for x-axis.\n",
    "\n",
    "    y : array_like\n",
    "        Data point values for y-axis. Note X and y should have the same size.\n",
    "    \"\"\"\n",
    "    fig = pyplot.figure()\n",
    "    pyplot.plot(X, y, \"ro\", ms=10, mec=\"k\")\n",
    "    pyplot.ylabel(\"Profit in $10,000\")\n",
    "    pyplot.xlabel(\"Population of City in 10,000s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "executionInfo": {
     "elapsed": 562,
     "status": "ok",
     "timestamp": 1694527360494,
     "user": {
      "displayName": "Arthur C.",
      "userId": "18023806256318143056"
     },
     "user_tz": 180
    },
    "id": "lUPaSCnzqK3w",
    "outputId": "dc6b15f0-0e0f-401c-9c94-67e748a86a9a"
   },
   "outputs": [],
   "source": [
    "plotData(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88Q9ejcFqK4G"
   },
   "source": [
    "### Gradient Descent\n",
    "\n",
    "In this part, we will adjust the $\\theta$ parameters of the linear regression of our dataset using the technique called *gradient descent*.\n",
    "\n",
    "All the variables used in the equations for this project follow the same pattern used in sections 1 and 2 of Source 1 of the course “[Machine Learning](https://www.coursera.org/learn/machine-learning)”, available on the Coursera platform.\n",
    "\n",
    "#### Updating the equations\n",
    "\n",
    "The aim of linear regression is to minimize the cost function\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m \\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right)^2$$\n",
    "\n",
    "where the hypothesis $h_\\theta(x)$ is given by the linear model\n",
    "\n",
    "$$ h_\\theta(x) = \\theta^Tx = \\theta_0 + \\theta_1 x_1$$\n",
    "\n",
    "Note that the parameters of our model are the values of $\\theta_j$. These values must be adjusted to minimize the cost function $J(\\theta)$. One way of doing this is to use the *gradient descent* method on *batches* of data. In this technique, each iteration performs an update\n",
    "\n",
    "$$ \\theta_j = \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta(x^{(i)}) - y^{(i)}\\right)x_j^{(i)} \\qquad \\text{ simultaneous update of } \\theta_j \\text{ for all } j$$\n",
    "\n",
    "With each iteration of the method, the parameter $\\theta_j$ approaches the optimal values, i.e. those at which the cost function $J(\\theta)$ will be minimized.\n",
    "\n",
    "*Implementation note: We store each example in a row of the $X$ matrix. To take into account the intercept term ($\\theta_0$), we will add an “extra” first column to $X$, in which all elements will be equal to '1'. This will allow us to treat $\\theta_0$ as another simple feature.*\n",
    "\n",
    "\n",
    "#### Implementation\n",
    "\n",
    "We already have the data for the linear regression. In the next cell, we will add another dimension to our data to accommodate the intercept term $\\theta_0$. Do NOT run this cell more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1694527360507,
     "user": {
      "displayName": "Arthur C.",
      "userId": "18023806256318143056"
     },
     "user_tz": 180
    },
    "id": "rcqZSb5RqK4H"
   },
   "outputs": [],
   "source": [
    "# Add a column of ones to X. The numpy function stack joins arrays along a given axis.\n",
    "# The first axis (axis=0) refers to rows (training examples)\n",
    "# and second axis (axis=1) refers to columns (features).\n",
    "X = np.stack([np.ones(m), X], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fyi2gyBkqK4M"
   },
   "source": [
    "#### Calculating the cost $J(\\theta)$\n",
    "\n",
    "When we use the *gradient descent* method to minimize the cost function $J(\\theta)$, it is interesting to monitor its convergence, i.e. the value of the cost over the iterations. In this section, we will implement the function that calculates $J(\\theta)$, allowing us to monitor the convergence of our implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1694527360508,
     "user": {
      "displayName": "Arthur C.",
      "userId": "18023806256318143056"
     },
     "user_tz": 180
    },
    "id": "FUgYSoCkqK4O"
   },
   "outputs": [],
   "source": [
    "def computeCost(X, y, theta):\n",
    "    \"\"\"\n",
    "    Compute cost for linear regression. Computes the cost of using theta as the\n",
    "    parameter for linear regression to fit the data points in X and y.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        The input dataset of shape (m x n+1), where m is the number of examples,\n",
    "        and n is the number of features. We assume a vector of one's already\n",
    "        appended to the features so we have n+1 columns.\n",
    "\n",
    "    y : array_like\n",
    "        The values of the function at each data point. This is a vector of\n",
    "        shape (m, ).\n",
    "\n",
    "    theta : array_like\n",
    "        The parameters for the regression function. This is a vector of\n",
    "        shape (n+1, ).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    J : float\n",
    "        The value of the regression cost function.\n",
    "    \"\"\"\n",
    "    m = y.size  # Number of training examples\n",
    "    J = 0\n",
    "    hypothesis = np.dot(X, theta)\n",
    "    squared_errors = (hypothesis - y) ** 2\n",
    "    J = (1 / (2 * m)) * np.sum(squared_errors)\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_L5tocIqK4U"
   },
   "source": [
    "Once the function is complete, the next step is to run `computeCost` twice using two different initializations of $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1694527360508,
     "user": {
      "displayName": "Arthur C.",
      "userId": "18023806256318143056"
     },
     "user_tz": 180
    },
    "id": "Gm5Juq_EqK4V",
    "outputId": "62a7418d-f130-4ea7-fc2a-051db1b2fb8d"
   },
   "outputs": [],
   "source": [
    "J = computeCost(X, y, theta=np.array([0.0, 0.0]))\n",
    "print(\"With theta = [0, 0] \\nCalculated cost = %.2f\" % J)\n",
    "print(\"Expected cost value (approximate) 32.07\\n\")\n",
    "\n",
    "# Further testing of the cost function\n",
    "J = computeCost(X, y, theta=np.array([-1, 2]))\n",
    "print(\"With theta = [-1, 2]\\nCalculated cost = %.2f\" % J)\n",
    "print(\"Expected cost value (approximate) 54.24\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uKIQxVYjqK4a"
   },
   "source": [
    "#### Gradient Descent\n",
    "\n",
    "Next, we will implement the function that describes the gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1694527360509,
     "user": {
      "displayName": "Arthur C.",
      "userId": "18023806256318143056"
     },
     "user_tz": 180
    },
    "id": "cEGqfLXsqK4b"
   },
   "outputs": [],
   "source": [
    "def gradientDescent(X, y, theta, alpha, num_iters):\n",
    "    \"\"\"\n",
    "    Performs gradient descent to learn `theta`. Updates theta by taking `num_iters`\n",
    "    gradient steps with learning rate `alpha`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        The input dataset of shape (m x n+1).\n",
    "\n",
    "    y : array_like\n",
    "        Value at given features. A vector of shape (m, ).\n",
    "\n",
    "    theta : array_like\n",
    "        Initial values for the linear regression parameters.\n",
    "        A vector of shape (n+1, ).\n",
    "\n",
    "    alpha : float\n",
    "        The learning rate.\n",
    "\n",
    "    num_iters : int\n",
    "        The number of iterations for gradient descent.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    theta : array_like\n",
    "        The learned linear regression parameters. A vector of shape (n+1, ).\n",
    "\n",
    "    J_history : list\n",
    "        A python list for the values of the cost function after each iteration.\n",
    "    \"\"\"\n",
    "    m = y.shape[0]  # Number of training examples\n",
    "    dim = theta.shape[0]  # Number of parameters\n",
    "    # Make a copy of theta to avoid changing the original array\n",
    "    theta = theta.copy()\n",
    "    J_history = []\n",
    "    for i in range(num_iters):\n",
    "        hypothesis = np.dot(X, theta)\n",
    "        error = hypothesis - y\n",
    "        gradient = (1 / m) * np.dot(X.T, error)\n",
    "        theta -= alpha * gradient\n",
    "        J_history.append(computeCost(X, y, theta))\n",
    "    return theta, J_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y6SLs1dlqK4j"
   },
   "source": [
    "Now we'll call the implemented function `gradientDescent` and print out the calculated $\\theta$. We will initialize the parameters $\\theta$ to 0 and the learning rate $\\alpha$ to 0.01. Run the next cell to check the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1694527360509,
     "user": {
      "displayName": "Arthur C.",
      "userId": "18023806256318143056"
     },
     "user_tz": 180
    },
    "id": "v_xL4YnGqK4k",
    "outputId": "78fdb4b0-fc16-4238-f554-50799af92efb"
   },
   "outputs": [],
   "source": [
    "# Initialize fitting parameters\n",
    "theta = np.zeros(2)\n",
    "# Gradient descent settings\n",
    "iterations = 1500\n",
    "alpha = 0.01\n",
    "\n",
    "theta, J_history = gradientDescent(X, y, theta, alpha, iterations)\n",
    "print(\"Theta found with gradient descent: {:.4f}, {:.4f}\".format(*theta))\n",
    "print(\"Expected theta values (approximate): [-3.6303, 1.1664]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQMWJ72tqK4q"
   },
   "source": [
    "We will use the parameters obtained as output from the `gradientDescent` function to plot the linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "executionInfo": {
     "elapsed": 569,
     "status": "ok",
     "timestamp": 1694527361055,
     "user": {
      "displayName": "Arthur C.",
      "userId": "18023806256318143056"
     },
     "user_tz": 180
    },
    "id": "J3A3VDzvqK4s",
    "outputId": "d9ded8e6-ab53-4cc2-ad8c-bc23c8cb8374"
   },
   "outputs": [],
   "source": [
    "# Plot the linear fit\n",
    "plotData(X[:, 1], y)\n",
    "pyplot.plot(X[:, 1], np.dot(X, theta), \"-\")\n",
    "pyplot.legend([\"Training data\", \"Linear regression\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQHs7nNYqK4x"
   },
   "source": [
    "Our final $\\theta$ figures will also be used to forecast profits in areas of 35 and 70 thousand inhabitants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1694527361056,
     "user": {
      "displayName": "Arthur C.",
      "userId": "18023806256318143056"
     },
     "user_tz": 180
    },
    "id": "oAs7gX7cqK4y",
    "outputId": "a4638142-d6c4-4e57-98d9-d82df3e27580"
   },
   "outputs": [],
   "source": [
    "# Predict values for population sizes of 35,000 and 70,000\n",
    "predict1 = np.dot([1, 3.5], theta)\n",
    "print(\n",
    "    \"For population = 35,000, we forecast a profit of {:.2f}\\n\".format(\n",
    "        predict1 * 10000\n",
    "    )\n",
    ")\n",
    "\n",
    "predict2 = np.dot([1, 7], theta)\n",
    "print(\n",
    "    \"For population = 70,000, we forecast a profit of {:.2f}\\n\".format(\n",
    "        predict2 * 10000\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pInUewKOqK45"
   },
   "source": [
    "### Visualizing $J(\\theta)$\n",
    "\n",
    "To better understand the cost function $J(\\theta)$, we will now plot the cost on a three-dimensional graph, with the cost on the z-axis as a function of $\\theta_0$ and $\\theta_1$. The purpose of these graphs is to show how $J(\\theta)$ varies with changes in $\\theta_0$ and $\\theta_1$. The cost function $J(\\theta)$ has a global minimum. This is easier to visualize in the contour plot than in the 3-D plot. This minimum is the optimum point for $\\theta_0$ and $\\theta_1$, and each iteration of the *gradient descent* method moves the parameters closer to this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "executionInfo": {
     "elapsed": 1437,
     "status": "ok",
     "timestamp": 1694527362479,
     "user": {
      "displayName": "Arthur C.",
      "userId": "18023806256318143056"
     },
     "user_tz": 180
    },
    "id": "7r-lIe9XqK46",
    "outputId": "d4efe9c0-7b34-4b11-acd4-6deb9b56bb24"
   },
   "outputs": [],
   "source": [
    "# grid over which we will calculate J\n",
    "theta0_vals = np.linspace(-10, 10, 100)\n",
    "theta1_vals = np.linspace(-1, 4, 100)\n",
    "\n",
    "# initialize J_vals to a matrix of 0's\n",
    "J_vals = np.zeros((theta0_vals.shape[0], theta1_vals.shape[0]))\n",
    "\n",
    "# Fill out J_vals\n",
    "for i, theta0 in enumerate(theta0_vals):\n",
    "    for j, theta1 in enumerate(theta1_vals):\n",
    "        J_vals[i, j] = computeCost(X, y, [theta0, theta1])\n",
    "\n",
    "# Because of the way meshgrids work in the surf command, we need to\n",
    "# transpose J_vals before calling surf, or else the axes will be flipped\n",
    "J_vals = J_vals.T\n",
    "\n",
    "# surface plot\n",
    "fig = pyplot.figure(figsize=(12, 5))\n",
    "ax = fig.add_subplot(121, projection=\"3d\")\n",
    "ax.plot_surface(theta0_vals, theta1_vals, J_vals, cmap=\"viridis\")\n",
    "pyplot.xlabel(\"theta0\")\n",
    "pyplot.ylabel(\"theta1\")\n",
    "pyplot.title(\"Surface\")\n",
    "\n",
    "# contour plot\n",
    "# Plot J_vals as 15 contours spaced logarithmically between 0.01 and 100\n",
    "ax = pyplot.subplot(122)\n",
    "pyplot.contour(\n",
    "    theta0_vals,\n",
    "    theta1_vals,\n",
    "    J_vals,\n",
    "    linewidths=2,\n",
    "    cmap=\"viridis\",\n",
    "    levels=np.logspace(-2, 3, 20),\n",
    ")\n",
    "pyplot.xlabel(\"theta0\")\n",
    "pyplot.ylabel(\"theta1\")\n",
    "pyplot.plot(theta[0], theta[1], \"ro\", ms=10, lw=2)\n",
    "pyplot.title(\"Contour, showing the point of minimum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-ZEkvux7RmF"
   },
   "source": [
    "Finally, it is interesting to observe the evolution of the cost function obtained throughout the iterations of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1694527362479,
     "user": {
      "displayName": "Arthur C.",
      "userId": "18023806256318143056"
     },
     "user_tz": 180
    },
    "id": "LOkRp3Sg_nyw"
   },
   "outputs": [],
   "source": [
    "def plot_J_history(J_history):\n",
    "    \"\"\"\n",
    "    Plot the evolution of the cost function over the iterations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    J_history : list\n",
    "        list with J values.\n",
    "    \"\"\"\n",
    "    fig = pyplot.figure()\n",
    "    pyplot.ylabel(\"Computed cost (J)\")\n",
    "    pyplot.xlabel(\"Number of iterations\")\n",
    "    pyplot.plot(J_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "executionInfo": {
     "elapsed": 373,
     "status": "ok",
     "timestamp": 1694527362849,
     "user": {
      "displayName": "Arthur C.",
      "userId": "18023806256318143056"
     },
     "user_tz": 180
    },
    "id": "l5M6N__Z62it",
    "outputId": "bbfcc8e6-cbbe-49ef-f53e-c156f03563d6"
   },
   "outputs": [],
   "source": [
    "plot_J_history(J_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmitUIauB0TL"
   },
   "source": [
    "Notice how the cost function converges to smaller values than the initial ones, indicating the evolution of the regression fit over the iterations of the algorithm."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
